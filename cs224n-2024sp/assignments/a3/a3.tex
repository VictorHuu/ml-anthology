\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{fancyhdr} 
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{array}
\usepackage{booktabs}
\setlength{\headheight}{14pt} 

\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{CS224N Winter 2026}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0pt}

\title{Assignment 3 Self-Attention and Transformers}
\author{Yuntao Hu}
\date{}

\begin{document}
	
	\maketitle
	\thispagestyle{plain}
	\section{Attention Exploration}
	\begin{enumerate}
	\item[a] 
	\begin{enumerate}
	\item[i]If $a_j \gg \sum_{i\ne j} a_i$ holds,then we have
	\begin{eqnarray*}
	&&\exp{(k_j^Tq)}\gg \sum_{i\ne j}\exp{(k_i^Tq)}\\
	&&k_j^Tq \gg \log{\left(\sum_{i\ne j}\exp{(k_i^Tq)}\right)}\quad (2)
	\end{eqnarray*}
	And the right-hand side of the 2-nd inequality can be seen as the analytical solution of $\max_{i\ne j} {k_i^Tq}$.Intuitively, this means that $k_j$ is much more similar to $q$ than any other $k_i,i\ne j$.
	
	After all,the inequality $(2)$ must hold.
	\item[ii] $c\approx v_j$
	\end{enumerate}
	\item[b] According to (a), if we want $c=\frac{1}{2}(v_a+v_b)$,then we let $\exp\left(k_a^Tq\right)+\exp\left(k_b^Tq\right)\gg \sum_{i\ne a \bigcap i\ne b} k_i^Tq$ while $k_a^Tq=k_b^Tq$.
	Since all of the $k$ are orthogonal,let $q=\lambda(k_a+k_b)$.Obviously,$(k_a^T+k_b^T)q\ne 0$,$\sum_{i\ne a \bigcap i\ne b} k_i^Tq=0$ and $\alpha_a=\alpha_b=\frac{1}{2}$
	\item[c] 
	\begin{enumerate}
	\item[i] And we have $k_a\sim N(\mu_a,\alpha I),k_b\sim N(\mu_b,\alpha I)$,so $k_a+k_b=\mu_a+\mu_b+\epsilon$ where $\epsilon \sim N(0,2\alpha I)$.Then we design $ q=\mu_a+\mu_b$.
	
	Since $\alpha$ is vanishingly small ,$k_a\approx \mu_a$,$k_b \approx \mu_b$,
	$k_a^Tq\approx \mu_a^T(\mu_a+\mu_b)=\|\mu_a\|_2^2=1$,$k_b^Tq\approx \mu_b^T(\mu_a+\mu_b)=\|\mu_b\|_2^2=1$,then $\alpha_a=\alpha_b=\frac{1}{2}$.Finally,$c\approx \frac{1}{2}(v_a+v_b)$
	\item[ii] $k_a\sim N(\mu_a,\alpha I+\frac{1}{2}\mu_a\mu_a^T)$,notice that $r(\mu_a\mu_a^T)=1$,$\mu_a^Tk_a\sim N(1,\alpha \mu_a^T +\frac{1}{2}\mu_a)$.Since $\alpha$ is vanishingly small, we can approximately assume that $\mu_a^Tk_a\sim N(1,\frac{1}{2}\mu_a)$.For all other $mu_i,i\ne a$,we have $\mu_i ^T k_a\sim N(1,0)$.
	
	According to $(1)$,we have $q=\lambda(\mu_a+\mu_b)$,then we have $q^Tk_a\approx \lambda(\mu_a^Tk_a)$ and $q^Tk_b \approx \lambda$.
	The weight $w_a=\frac{e^{s_a}}{e^{s_a}+e^{s_b}}=\sigma (s_a-s_b)$ while $s_a-s_b \sim O(\lambda)$. So $s_a$ and $s_b$ will no longer be almost the same,it's more likely that either $s_a$ or $s_b$ will be apparently larger.So either $c\approx v_a$ or $c\approx v_b$ will hold in most cases.
	\end{enumerate}
	\item[d] 
	\begin{enumerate}
	\item[i]$q_1=\lambda\mu_a$,$q_2=\lambda\mu_b$
	\item[ii] Using the same queries $q_1=\lambda\mu_a$ and $q_2=\lambda\mu_b$. For head 1,
	\[
	s_a^{(1)} = q_1^\top k_a = \lambda\,\mu_a^\top k_a,
	\]
	and under $\Sigma_a=\alpha I + \tfrac12 \mu_a\mu_a^\top$ (with $\alpha\to 0$), the scalar $\mu_a^\top k_a$ has large sample-to-sample variation, so $s_a^{(1)}$ varies substantially across samples (on the order of $\lambda$). Hence the softmax weights in head 1 change noticeably from sample to sample: when $\mu_a^\top k_a$ is large, $w_a^{(1)}\approx 1$ and $c_1\approx v_a$; when it is smaller, $w_a^{(1)}$ is less dominant and $c_1$ mixes in other $v_i$. (We ignore cases where $q_1^\top k_a<0$ as allowed.)
	
	For head 2,
	\[
	s_b^{(2)} = q_2^\top k_b \approx \lambda
	\]
	with negligible variation since $\Sigma_b=\alpha I$, so $c_2\approx v_b$ remains stable.
	
	Therefore, the final output
	\[
	c=\tfrac12(c_1+c_2)
	\]
	fluctuates mainly due to head 1. Since $c_2$ is nearly deterministic, and ignoring any covariance between heads,
	\[
	\mathrm{Var}(c)
	=\tfrac14\!\left(\mathrm{Var}(c_1)+\mathrm{Var}(c_2)+2\,\mathrm{Cov}(c_1,c_2)\right)
	\approx \tfrac14\,\mathrm{Var}(c_1),
	\]
	which is qualitatively lower-variance (and less ``switchy'') than the single-head construction in 1(c) where $v_a$ and $v_b$ compete within the same softmax.
	\end{enumerate}
	\item[e]Multi-headed attention avoids having $a$ and $b$ compete inside one softmax: separate heads attend to $a$ and $b$ independently, then average, so variance from a noisy/norm-fluctuating key (e.g., ($k_a$)) doesnâ€™t flip the whole output and the final $c$ is much more robust than the single-head case.
	
	\end{enumerate}
	\section{Position Embed-dings Exploration}
	\begin{enumerate}
	\item[a]
	\begin{enumerate}
	\item[i] $H=\mathbf{softmax}(\frac{PQ{(PK)}^T}{\sqrt{d}})V=\mathbf{softmax}(\frac{PQK^TP^T}{\sqrt{d}})V=P\mathbf{softmax}(\frac{QK^T}{\sqrt{d}})P^TV$,so $S_{perm}=PSP^T$. Then,$H_{perm}=P\mathbf{softmax}(S)P^TPV=P\mathbf{softmax}(S)V=PH$.
	
	According to $\mathbf{ReLU}(PA)=P\mathbf{ReLU}(A)$,so $Z_{perm}=PZ$
	\item[ii] Because the model is permutation equivariant (i.e., permuting the input tokens just permutes the outputs in the same way), it is insensitive to word order.
	\end{enumerate}
	\item[b] 
	\begin{enumerate}
	\item[i] \textbf{Yes},because $PX+\Phi\ne PX_{pos}$,the position information encompassed in the embedding preserves.Thus it has become sensitive to position.
	\item[ii] \textbf{No}. There is no finite least common multiple. The periods are $T_i=2\pi 10000^{\frac{2i}{d}}$
	.
	For most $i\ne j$,$\frac{T_i}{T_j}=10^{8(i-j)/d}$ is irrational (since the exponent is typically non-integer). Hence the component frequencies are incommensurate, and the overall positional encoding is not periodic. So for every position $i$,the position embedding is unique.
	\end{enumerate}
	\end{enumerate}
\end{document}
