\documentclass{article}

\title{Lecture 5: GDA\& Naive Bayes}
\author{Yuntao Hu}
\date{}

\begin{document}
\maketitle
\section{Recap}

Discrimitive:
\[
p(y|x)
\]

Generarive Learing Algorithm:
\[
p(x|y)
\]
$x$ is feature, $y$ is class ,$p(y)$ is class-prior,using Bayes Rule:
\[
P(y=1|x)=\frac{P(x|y=1)P(y=1)}{P(x)}
\]
\[
p(x)=p(x|y=1)p(y=1)+p(x|y=0)p(y=0)
\]
\section{Gaussian Discrimitive Analysis}

Suppose $x\in \mathcal{R}^n$(drop $x_0=1$ convention)

Assume $p(x|y)$ is Gaussian

Let's talk about 2-dimentional:$z \sim N(\mu,\Sigma),z\in \mathcal{R}^n,\mu \in \mathcal{R}^n,\Sigma\in \mathcal{R}^{n \times n}$

Then we have 
\[E[z]=\mu
\]
\[
Cov(z)=E[(z-\mu)(z-\mu)^T]=Ezz^T-(Ez)(Ez)^T
\]

We will give the density:
\[
p(x;\mu,\Sigma)=\frac{1}{(2\pi)^{2\over d}\Sigma^{1\over2}}exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)
\]
\begin{enumerate}
\item[(a)]When we reduce the diagonal element of $\Sigma$, the spread-out of the density will be reduced(i.e. taller)
\item[(b)] When we scale up the off-diagonal element,if it's positive, it will be more flattened along one diagonal
\item[(b)] When we scale down the off-diagonal element,if it's negative, it will be more flattened along another diagonal
\end{enumerate}
\[
P(x|y=0)=\frac{1}{(2\pi)^{2\over d}\Sigma^{1\over2}}exp\left(-\frac{1}{2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0)\right)
\]
\[
P(x|y=1)=\frac{1}{(2\pi)^{2\over d}\Sigma^{1\over2}}exp\left(-\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\right)
\]
Then we knew $P(y)$

\[
P(y)=\phi^y(1-\phi)^{1-y}
\]

There's a training set $\{x^{(i)},y^{(i)}\}_{i=1}^M$,we are going to maximize the joint likelihood:
\[
\mathcal{L}(\phi,\mu_0,\mu_1,\Sigma)=\prod_{i=1}^m \mathcal{P}(x^{(:)},y^{(:)};\phi,\mu_0,\mu_1)
\]
Discrimitive Learning Algorithm:
\[
\mathcal{L}(\theta)=\prod_{i=1}^m P(y^{(:)}|x^{(:)};\theta)
\]
We use maximum likelihood estimation:
\[
\max_{\phi,\mu_0,\mu_1,\Sigma}l(\phi,\mu_0,\mu_1,\Sigma)=\log \mathcal{L}(...)
\]
Then we got
\begin{enumerate}
\item[I] $\phi=\frac{\sum_{i=1}^m 1{y^{(i)}=1} }{m}$(the fraction of label y equals to 1)
\item[II] $\mu_0 =\frac{\sum_{i=1}^m 1\{y^{(i)}=0\}x^{(i)}}{\sum_{i=1}^m 1\{y^{(i)}=0\}}$
\item[III] $\mu_1 =\frac{\sum_{i=1}^m 1\{y^{(i)}=1\}x^{(i)}}{\sum_{i=1}^m 1\{y^{(i)}=1\}}$
\item[IV] $\Sigma=\frac{1}{m} \sum_{i=1}^m (x^{(i)}-\mu_{y^{i}})(x^{(i)}-\mu_{y^{i}})^T$
\end{enumerate}
Prediction:
\[
\max_y P(y|x)=arg \max_y \frac{P(x|y)P(y)}{P(x)}=arg \max_y P(x|y)P(y)
\]
Comparison of Logistic Regression and GDA

Logistic Regression
\begin{enumerate}

\item[I]Discriminative

\item[II]Fewer assumptions, more robust

\item[III]Needs more data
\end{enumerate}
GDA
\begin{enumerate}
\item[I]Generative

\item[II]Strong assumptions, data-efficient

\item[III]Sensitive to assumption violations
\end{enumerate}
GDA assumptions can imply logistic regression ,but not vice versa.
\begin{enumerate}
\item $x|y=0 \sim \mathcal{N}(\mu_0,\Sigma)$
\item $x|y=1 \sim \mathcal{N}(\mu_1,\Sigma)$
\item $y \sim \mathcal{B}(\phi)$
\end{enumerate}
The above is those of GDA, which is stronger.

Then we try to generalize distributions of $x|y=0$ and $x|y=1$ to Poisson,etc. We can still get logistic regression.
\end{document}