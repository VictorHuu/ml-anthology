\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{fancyhdr} 
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{subcaption}
\setlength{\headheight}{14pt} 

\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{CS229 Solution Set \#2}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0pt}

\title{Solution Set 2: Supervised Learning II}
\author{Yuntao Hu}
\date{}

\begin{document}

\maketitle
\thispagestyle{plain}

\section{Logistic Regression: Training stability}

\begin{itemize}
    \item[(a)] training on the dataset B doesn't converge in a long time.
    \item[(b)] Because dataset B is linearly separable and so the result theoretically will be infinity.
    
    \textbf{Proof:} Since B is linearly separable $y^{(i)}(\theta^{T}x^{(i)}+b)>0$ always holds. To make the functional margin large (the result more confident), $\theta^{T}$ needs to scale up infinitely, and the probability becomes extremely near 0, thus the gradient also becomes even smaller, so the training process becomes even slower.
    
    A is not linearly separable, so there will be some points where $y^{(i)} = y^{(i)}(\theta^{T}x^{(i)}+b)<0$, the functional margin will be negative, so we can quickly find the optimal margin without worrying about the vanishing gradient.
    
    \begin{itemize}
        \item[i] \textbf{No.} Since the functional margin won't reach the global minimum at a finite value (non-convex optimization), using a different constant learning rate won't help much. Note that $||w||$ will become larger as the training goes, which means we should be careful even if learning rate is fast enough that $||w||$ will be exploded. But if you take a small rate, the process will be even slower.
        
        \item[ii] \textbf{Yes.} This is an adaptive approach. As the gradient descent iterates, the learning rate $\frac{1}{t^{2}}$ becomes extremely smaller. When it decreases to the same order of $10^{-15}$ it will stop at some iteration.
        
        \item[iii] \textbf{No.} Linear scaling won't change the contour and thus it's still linearly separable.
        
        \item[iv] \textbf{Yes.} Adding a regularization term definitely makes the objective a convex function w.r.t. $\theta$, it will definitely reach the global minimum at a finite value.
        
        \item[v] \textbf{Likely yes.} It's likely that the training data may be linearly inseparable.
    \end{itemize}

    \item[(c)] \textbf{No.} Because SVM w/ the hinge loss $J(\theta)=\max\{0,1-z\}$ is more robust. It only cares about the data adjacent to the border/margin, and even if the training data is linearly separable, since $z$ for all of the data will soon become greater than 1 ($z$ is functional margin so it can scale), thus $J(\theta)$ becomes zero and comes to convergence.
\end{itemize}

\newpage

\section{Model Calibration}

\begin{itemize}
    \item[(a)] The loss function $J(\theta)$ for logistic regression:
    \[ J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}\log(h_{\theta}(x^{(i)}))+(1-y^{(i)})\log(1-h_{\theta}(x^{(i)}))] \]
    
    Take the derivative:
    \[ \frac{\partial J(\theta)}{\partial\theta_{j}}=\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)} \]
    
    Assume that the bias $\theta_{0}=\alpha$, and since $x_{0}^{(i)}=1$ for all $i=1,\dots,m$:
    \[ \frac{\partial J(\theta)}{\partial\theta_{0}}=\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)}) \]
    
    Then we get the equation when set the partial derivative to 0:
    \[ \frac{1}{m}\sum_{i=1}^{m}h_{\theta}(x^{(i)})=\frac{1}{m}\sum_{i=1}^{m}y^{(i)} \]
    
    This can also be written as:
    \[ \frac{1}{|I_{0,1}|}\sum_{i\in I_{0,1}}P(y=1|x;\theta)=\frac{1}{|I_{0,1}|}\sum_{i\in I_{0,1}}I(y^{(i)}=1) \]
    Q.E.D.

    \item[(b)] \textbf{No.} Because the calibration only guarantees the equation above. That means when there are 50\% data whose $y$ is 1, and all of the predicted values are 0.5. It does calibrate well for any $(a,b)\subset[0,1]$ where $a\le0.5\le b$. But this model fails completely on the dataset.
    
    \textbf{No.} Because when it comes to prediction, any value greater than 0.5 means positive. Think about all of the predicted value is 0.6 and it has an 100\% accuracy, it doesn't calibrate well since $0.6\ne1$.

    \item[(c)] The loss function w/ L2 regularization is:
    \[ J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}\log(h_{\theta}(x^{(i)}))+(1-y^{(i)})\log(1-h_{\theta}(x^{(i)}))]+\frac{\lambda}{2m}||\theta||^{2} \]
    
    Take the derivative w.r.t. $\theta_{0}$:
    \[ \frac{\partial J(\theta)}{\partial\theta_{0}}=\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})+\frac{\lambda}{m}\alpha \]
    
    And obviously it doesn't calibrate well:
    \[ \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})+\lambda\alpha)=\frac{1}{m}\sum_{i=1}^{m}y^{(i)} \]
\end{itemize}
\newpage

\section{Bayesian Interpretation of Regularization}
\begin{itemize}
 \item[(a)] 
\begin{eqnarray*}
\theta_{MAP}\\
&=&\arg \max_{\theta} p(\theta|x,y)\\
&=& \arg \max_{\theta}\frac{p(y|x,\theta)p(\theta|x)}{p(y|x)}\\
&=& \arg \max_{\theta}p(y|x,\theta)p(\theta|x)\\
&=&\arg \max_{\theta}p(y|x,\theta)p(\theta)
\end{eqnarray*}
\item[(b)]
plug $p(\theta)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{\theta^2}{2\sigma^2}}$ into the above formula,we will get
\begin{eqnarray*}
\theta_{MAP}\\
&=&\arg \max_{\theta}p(y|x,\theta)\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{\theta^T\theta}{2\sigma^2}}\\
&=&\arg \max_{\theta}\log p(y|x,\theta)-\frac{1}{2}\log{2\pi\sigma^2}-\frac{\theta^T\theta}{2\sigma^2}\\
&=&\arg \min_{\theta}-\log p(y|x,\theta)+\frac{\theta^T\theta}{2\sigma^2}\\
&=&\arg \min_{\theta}-\log p(y|x,\theta)+{\left(\frac{1}{\sqrt{2}\sigma}\right)}^2{\|\theta\|}_2^2
\end{eqnarray*}
Let $\lambda={\left(\frac{1}{\sqrt{2}\sigma}\right)}^2$,we will get the final formula ,Q.E.D.
\item[(c)]
Since $\epsilon\sim \mathbf{N}(0,\sigma^2)$ and $y=\theta^Tx+\epsilon$,then we have
\begin{eqnarray*}
y|x,\theta \sim \mathbf{N}(\theta^Tx,\sigma^2)
\end{eqnarray*}
Then  plug it into the $\theta_{MAP}$ formula,we will get
\begin{eqnarray*}
\theta_{MAP}\\
&=&\arg \min_{\theta}-\sum_{i=1}^m\log p(y^{(i)}|x^{(i)},\theta)+{\left(\frac{1}{\sqrt{2}\eta}\right)}^2{\|\theta\|}_2^2\\
&=&\arg \min_{\theta}\sum_{i=1}^m\left(\frac{1}{2}\log {\left(2\pi\sigma^2\right)}+\frac{(y-x^T\theta)^2}{2\sigma^2}\right)+{\left(\frac{1}{\sqrt{2}\eta}\right)}^2{\|\theta\|}_2^2\\
&=&\arg \min_{\theta}\sum_{i=1}^m\left(\frac{(y-x^T\theta)^2}{2\sigma^2}\right)+{\left(\frac{1}{\sqrt{2}\eta}\right)}^2{\|\theta\|}_2^2\\
&=&\arg \min_{\theta}\sum_{i=1}^m(y-x^T\theta)^2+{\left(\frac{\sigma}{\eta}\right)}^2{\|\theta\|}_2^2\\
&=&\arg \min_{\theta} (y-X^T\theta)^T(y-X^T\theta)+{\left(\frac{\sigma}{\eta}\right)}^2\theta^T\theta\\
&=&\arg \min_{\theta} y^Ty-2\theta^TXy+\theta^TXX^T\theta+{\left(\frac{\sigma}{\eta}\right)}^2\theta^T\theta\\
&=&\arg \min_{\theta} -2\theta^TXy+\theta^TXX^T\theta+{\left(\frac{\sigma}{\eta}\right)}^2\theta^T\theta\\
&=&\arg \min_{\theta} \theta^T(-2Xy+XX^T\theta+{\left(\frac{\sigma}{\eta}\right)}^2\theta)
\end{eqnarray*}
Take the derivative of it,then we get:
\begin{eqnarray*}
-2Xy+2XX^T\theta+2{\left(\frac{\sigma}{\eta}\right)}^2\theta
\end{eqnarray*}
Then it reduces:
\begin{eqnarray*}
Xy=(XX^T+{\left(\frac{\sigma}{\eta}\right)}^2I)\theta
\end{eqnarray*}
Finally we get
\begin{eqnarray*}
\theta_{MAP}={\left(XX^T+{\left(\frac{\sigma}{\eta}\right)}^2I\right)}^{-1}(Xy)
\end{eqnarray*}
\item[(d)]
\begin{eqnarray*}
\theta_{MAP}\\
&=&\arg \max_{\theta} p(y|x,\theta)p(\theta|x)\\
&=&\arg \max_{\theta} p(y|x,\theta)\frac{1}{2b}e^{-\frac{\|\theta\|}{b}}\\
&=&\arg \max_{\theta} \log p(y|x,\theta)-\log{2b}-\frac{\|\theta\|}{b}\\
&=&\arg \min_{\theta} \sum_{i=1}^m\left(\frac{1}{2}\log {\left(2\pi\sigma^2\right)}+\frac{(y-x^T\theta)^2}{2\sigma^2}\right)+\frac{\|\theta\|}{b}\\
&=&\arg \min_{\theta} \sum_{i=1}^m\left(\frac{(y-x^T\theta)^2}{2\sigma^2}\right)+\frac{\|\theta\|}{b}\\
&=&\arg \min_{\theta} {\|y-X^T\theta\|}^2+\frac{2\sigma^2}{b} {\|\theta\|_1}
\end{eqnarray*}
Let $\gamma$ be $\frac{2\sigma^2}{b}$Q.E.D.
\end{itemize}
\newpage
\section{Constructing Kernels}
Let $G_1=[K_1(x_i,x_j)]$ be the corresponding Gram matrix for $K_1$,and let $G_2=[K_2(x_i,x_j)]$ be the corresponding Gram matrix for $K_2$.And according to Mercer's Theorem,both $G_1$ and $G_2$ are semi-definite,that is $\forall z\in R^n,z^TG_1z\ge 0,z^TG_2z\ge 0$,$G_1\succeq 0,G_2\succeq 0$
\begin{itemize}
\item[(a)] \textbf{Yes.}The Gram matrix for $K$ is $G=G_1+G_2$,according to Mercer's Theorem,since $\forall z\in R^n,z^TGz=z^T(G_1+G_2)z=z^TG_1z+z^TG_2z\ge 0$,hence $G\succeq 0$,so $K$ is a kernel function.
\item[(b)] \textbf{Not necessarily.} Suppose that $K_1(x,z)=x^Tz,K_2(x,z)=2x^Tz$
\item[(c)] \textbf{Yes.} The Gram matrix for $K$ is $G=aG_1$,according to Mercer's Theorem,since $\forall z\in R^n,z^TGz=z^T(aG_1)z=az^TG_1z\ge 0$,hence $G\succeq 0$,so $K$ is a kernel function.
\item[(d)] \textbf{No.} The Gram matrix for $K$ is $G=aG_1$,according to Mercer's Theorem,since $\forall z\in R^n,z^TGz=z^T(aG_1)z=az^TG_1z\le 0$,hence $G\preceq 0$,so $K$ isn't a kernel function.
\item[(e)] \textbf{Yes.} The Gram matrix for $K$ is $G=G_1\circ G_2,c_{ij}=a_{ij}b_{ij}$,Since $G_1$ and $G_2$ are semi-definite,they can be decomposed into $G_1=\sum_{k=1}^r x_kx_k^H,G_2=\sum_{l=1}^s y_ly_l^H$,and $r=rank(G_1),s=rank(G_2)$.Then $G=G_1\circ G_2=(\sum_{k=1}^r x_kx_k^H)\circ (\sum_{l=1}^s y_ly_l^H)=\sum_{k=1}^r\sum_{l=1}^s (x_kx_k^H)\circ(y_ly_l^H)$ Now we need to prove that $(xx^H)\circ(yy^H)$ is PSD.
\begin{eqnarray*}
(xx^H)\circ(yy^H)_{ij}=(x_i \overline{x_j})(y_y\overline{y_j})=(x_iy_i)(\overline{x_jy_j})
\end{eqnarray*}
Thus, $(xx^H)\circ(yy^H)=zz^H\succeq 0$,hence $G=\sum_{k=1}^r\sum_{l=1}^s (x_kx_k^H)\circ(y_ly_l^H)\succeq 0$,so K is a kernel function.
\item[(f)] \textbf{Yes.} K(x,z)=f(x)f(z)=f(z)f(x)=K(z,x) so it satifies the commtative law.

Define a vector space 
\[
\mathbf{H} = \Big\{ \sum_i \alpha_i f(x_i) \;\big|\; \alpha_i \in \mathbb{R}, \; x_i \in \Xi \Big\}
\]
and its inner product
\[
\langle f(x), f(z) \rangle_{\mathbf{H}} = f(x) f(z).
\] 
Since 
\[
\langle f(x), f(z) \rangle_{\mathbf{H}} = \langle f(z), f(x) \rangle_{\mathbf{H}},
\]
\[
\langle a f(x) + b f(y), f(z) \rangle_{\mathbf{H}} = a f(x) f(z) + b f(y) f(z),
\]
and
\[
\langle f(x), f(x) \rangle_{\mathbf{H}} = f(x)^2 \ge 0,
\]
\(\mathbf{H}\) is a Hilbert space.


For arbitrary finite samples $\{x_1,\cdots,x_n\}$ and $c_i\in R$:
\begin{eqnarray*}
\sum_{i,j}c_ic_j K(x_i,x_j)=\sum_{i,j}f(x_i)f(x_j)={\|\sum_i c_if(x_i)\|}^2
\end{eqnarray*}
So the Gram matrix for $K$ is postive semi-definite,thus $K$ is a kernel function.
\item[(g)] \textbf{Yes.}Since $K_3$ is a kernel function,there exists a function called $\kappa,\mathbb{R}^n ->\mathbb{R}$ such that $K_3(x,z)=\kappa(x)^T\kappa(z)$, so $K(x,z)=K_3(\phi(x),\phi(z))={(\kappa\circ {\phi(x)})}^T{\kappa\circ {\phi(z)}}$.We can easily show that $\kappa \circ \phi$ corresponds to a Hibert space,and K satisfies the commutative law since $K(x,z)={(\kappa\circ {\phi(x)})}^T{\kappa\circ {\phi(z)}}={(\kappa\circ {\phi(z)})}^T{\kappa\circ {\phi(x)}}=K(z,x)$

For arbitrary finite samples $\{x_1,\cdots,x_n\}$ and $c_i\in R$:
\begin{eqnarray*}
\sum_{i,j}c_ic_j K(x_i,x_j)=\sum_{i,j}{(\kappa\circ {\phi(x_i)})}{(\kappa\circ {\phi(x_j)})}={\|\sum_i {(\kappa\circ {\phi(x_i)})}\|}^2
\end{eqnarray*}
So the Gram matrix for $K$ is postive semi-definite,thus $K$ is a kernel function.
\item[(h)]\textbf{Yes.}The Gram matrix for $K$ is $G=p(G),p(G)=\sum_{i} \alpha_i x^i,\alpha_i>0$,according to Mercer's Theorem and Schur's Theorem,since $\forall z\in R^n,z^TGz=z^Tp(G)z=z^T\sum_{i} \alpha_i G^{\circ n}z\ge 0$,hence $G\succeq 0$,so $K$ is a kernel function.
\end{itemize}
\newpage
\section{Kernelizing the Perceptron}
\begin{itemize}
\item[(a)]
According to reprensenter theorem $\theta^{(i)}=\sum_{j=1}^i \beta_j \phi(x^{(j)})$,notice that when $\theta_0=0$,all of $\beta_i=0$,and then we apply the kernel trick:
\begin{eqnarray*}
\theta=\sum_{j=1}^i \beta_j \phi(x^{(j)}) +\alpha(y^{(i+1)}-g(\theta^T\phi(x^{(i+1)}))) \phi(x^{(i+1)})
\end{eqnarray*}
Obviously,we have $\beta_{i+1}=\alpha(y^{(i+1)}-g(\theta^T\phi(x^{(i+1)})))$,then replacing $\theta$ by $\sum_{j=1}^i \beta_j \phi(x^{(j)})$,we get
\begin{eqnarray*}
\beta_{i+1}\\
&=&\alpha(y^{(i+1)}-g(\theta^T\phi(x^{(i+1)})))\\
&=&\alpha(y^{(i+1)}-g({\left(\sum_{j=1}^i \beta_j \phi(x^{(j)})\right)}^T\phi(x^{(i+1)})))\\
&=&\alpha(y^{(i+1)}-g(\sum_{j=1}^i \beta_j \phi(x^{(j)})^T\phi(x^{(i+1)})))\\
&=&\alpha(y^{(i+1)}-g(\sum_{j=1}^i \beta_j <\phi(x^{(j)}),\phi(x^{(i+1)})>))\\
&=&\alpha(y^{(i+1)}-g(\sum_{j=1}^i \beta_j K(x^{(j)},x^{(i+1)})))
\end{eqnarray*}
Thus,when $i\ge 1$,$\beta_{i+1}=\alpha(y^{(i+1)}-g(\sum_{j=1}^{i} \beta_j K(x^{(j)},x^{(i+1)})))$,$\theta^{(i)}=\sum_{j=1}^{(i)}\beta_j\phi(x^{(j)})$
\item[(c)]
\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/p05_dot_output.png}
        \caption{Dot product kernel}
        \label{fig:dot_product}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/p05_rbf_output.png}
        \caption{RBF kernel}
        \label{fig:rbf_kernel}
    \end{subfigure}
    \caption{Kernelizing the Perceptron with different kernel functions.}
    \label{fig:kernel_comparison}
\end{figure}
The dot product kernel fails because $C_1$ (red points) forms a non-convex, hollow distribution surrounding the convex set $C_2$ (blue points). By the Supporting Hyperplane Theorem, any hyperplane $H$ attempting to separate $C_2$ from $C_1$ would, when shifted to touch the boundary of $C_2$, act as a supporting hyperplane for $C_2$. However, because $C_2$ is contained within the convex hull of $C_1$, any such supporting hyperplane $H$ must necessarily intersect the distribution of $C_1$. Thus, no single hyperplane can exist that keeps all points of $C_2$ on one side and all points of $C_1$ on the other
\end{itemize}
\end{document}
